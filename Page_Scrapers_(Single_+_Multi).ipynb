{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcGeGZJ2gg9BolPysCAbo7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XZ-dD-LN9OQ4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import os  # Import the os module\n",
        "\n",
        "# Preprocess text: Remove special characters and normalize spaces\n",
        "def preprocess_text(text):\n",
        "    return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', text)).lower()\n",
        "\n",
        "def fetch_html(url):\n",
        "    \"\"\"\n",
        "    Fetches the HTML content of a given URL.\n",
        "\n",
        "    Args:\n",
        "      url: The URL to fetch.\n",
        "\n",
        "    Returns:\n",
        "      The HTML content as a string, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_content(html, element_type, parent_element=None):\n",
        "    \"\"\"\n",
        "    Parses the HTML content to extract text from specified elements.\n",
        "\n",
        "    Args:\n",
        "      html: The HTML content to parse.\n",
        "      element_type: The type of element to extract text from (e.g., 'div', 'p').\n",
        "      parent_element: (Optional) The parent element to search within.\n",
        "\n",
        "    Returns:\n",
        "      The extracted text as a string, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        if parent_element:\n",
        "            parent = soup.find(parent_element)\n",
        "            if not parent:\n",
        "                print(f\"No parent element <{parent_element}> found.\")\n",
        "                return None\n",
        "            elements = parent.find_all(element_type)\n",
        "        else:\n",
        "            elements = soup.find_all(element_type)\n",
        "\n",
        "        if elements:\n",
        "            content = \"\"\n",
        "            for element in elements:\n",
        "                content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "            return content\n",
        "        else:\n",
        "            print(f\"No elements of type <{element_type}> found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data, filename, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Saves the extracted content to a CSV file.\n",
        "\n",
        "    Args:\n",
        "      data: The content to save.\n",
        "      filename: The name of the CSV file.\n",
        "      element_type: The type of element the content was extracted from.\n",
        "      parent_element: The parent element of the extracted content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        file_exists = os.path.isfile(filename)\n",
        "\n",
        "        with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            if not file_exists:  # Write header only if file is new\n",
        "                writer.writerow([\"URL\", \"Content\", \"Timestamp\", \"Element Type\", \"Parent Element\"])\n",
        "            writer.writerow([target_url, data, datetime.now(), element_type, parent_element])\n",
        "        print(f\"Content saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to CSV: {e}\")\n",
        "\n",
        "def save_to_db(data, db_name, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Saves the extracted content to a SQLite database.\n",
        "\n",
        "    Args:\n",
        "      data: The content to save.\n",
        "      db_name: The name of the database file.\n",
        "      element_type: The type of element the content was extracted from.\n",
        "      parent_element: The parent element of the extracted content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''CREATE TABLE IF NOT EXISTS url_content\n",
        "                                 (url TEXT, content TEXT, timestamp TEXT, element_type TEXT, parent_element TEXT)''')\n",
        "        cursor.execute(\"INSERT INTO url_content VALUES (?, ?, ?, ?, ?)\",\n",
        "                         (target_url, data, datetime.now(), element_type, parent_element))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(f\"Content saved to {db_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to database: {e}\")\n",
        "\n",
        "# @title Single Page (Text Only)\n",
        "# @markdown **Step 1:** Select which elements to parse\n",
        "element_type = ''  #@param ['section', 'article', 'div', 'p', 'span', 'main', 'header', 'footer', 'nav', 'aside'] {allow-input: true}\n",
        "# @markdown &nbsp;&nbsp;&nbsp;&nbsp;<sub><sup>*(parent_element not required)*</sub></sup>\n",
        "parent_element = ''  #@param [\"main\", \"div\", \"body\"] {allow-input: true}\n",
        "# @markdown >\n",
        "# @markdown **Step 2:** Add the URL to be scraped\n",
        "target_url = \"https://www.stonebranch.com/it-automation-solutions/workload-automation\"  #@param {type:\"string\"}\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    html = fetch_html(target_url)\n",
        "    if html:\n",
        "        content = parse_content(html, element_type, parent_element)\n",
        "        if content:\n",
        "            # Save content to CSV and database with timestamps\n",
        "            save_to_csv(content, '/content/url_content.csv', element_type, parent_element)\n",
        "            save_to_db(content, '/content/url_content.db', element_type, parent_element)\n",
        "            print(f\"Content from {target_url} saved successfully at {datetime.now()}\")\n",
        "        else:\n",
        "            print(f\"No content found in <{element_type}> elements of {target_url}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch HTML from {target_url}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Preprocess text: Remove special characters and normalize spaces\n",
        "def preprocess_text(text):\n",
        "    return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', text)).lower()\n",
        "\n",
        "# List of common User-Agent strings (desktop & mobile)\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 16_0 like Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36\",\n",
        "]\n",
        "\n",
        "# Set up a session (helps with efficiency & maintains cookies)\n",
        "session = requests.Session()\n",
        "\n",
        "def fetch_html(url, max_retries=3):\n",
        "    \"\"\"\n",
        "    Fetches the HTML content of a given URL while rotating User-Agents.\n",
        "\n",
        "    Args:\n",
        "      url (str): The URL to fetch.\n",
        "      max_retries (int): Number of times to retry on failure.\n",
        "\n",
        "    Returns:\n",
        "      str: HTML content if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        headers = {\n",
        "            \"User-Agent\": random.choice(USER_AGENTS)  # Rotate User-Agent\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = session.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "            # Introduce a random delay between requests (2-6 seconds)\n",
        "            time.sleep(random.uniform(2, 6))\n",
        "\n",
        "            return response.text  # Return the page content\n",
        "\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            print(f\"[{attempt+1}/{max_retries}] HTTP error for {url}: {http_err}\")\n",
        "            if response.status_code == 403:\n",
        "                print(\"403 Forbidden! Rotating User-Agent and retrying...\")\n",
        "\n",
        "        except requests.exceptions.RequestException as req_err:\n",
        "            print(f\"[{attempt+1}/{max_retries}] Request error for {url}: {req_err}\")\n",
        "\n",
        "        time.sleep(random.uniform(3, 7))  # Wait before retrying\n",
        "\n",
        "    print(f\"Failed to fetch {url} after {max_retries} retries.\")\n",
        "    return None  # Return None if all retries fail\n",
        "\n",
        "def parse_content(html, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Parses the HTML content to extract text from specified elements.\n",
        "\n",
        "    Args:\n",
        "      html: The HTML content to parse.\n",
        "      element_type: The type of element to extract text from (e.g., 'div', 'p').\n",
        "      parent_element: (Optional) The parent element to search within.\n",
        "\n",
        "    Returns:\n",
        "      The extracted text as a string, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        if parent_element:\n",
        "            parent = soup.find(parent_element)\n",
        "            if not parent:\n",
        "                print(f\"No parent element <{parent_element}> found.\")\n",
        "                return None\n",
        "            elements = parent.find_all(element_type)\n",
        "        else:\n",
        "            elements = soup.find_all(element_type)\n",
        "\n",
        "        if elements:\n",
        "            content = \"\"\n",
        "            for element in elements:\n",
        "                content += element.get_text(separator=' ', strip=True) + \" \"\n",
        "            return content\n",
        "        else:\n",
        "            print(f\"No elements of type <{element_type}> found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data, filename, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Saves the extracted content to a CSV file.\n",
        "\n",
        "    Args:\n",
        "      data: The content to save.\n",
        "      filename: The name of the CSV file.\n",
        "      element_type: The type of element the content was extracted from.\n",
        "      parent_element: The parent element of the extracted content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        file_exists = os.path.isfile(filename)\n",
        "\n",
        "        with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            if not file_exists:  # Write header only if file is new\n",
        "                writer.writerow([\"URL\", \"Content\", \"Timestamp\", \"Element Type\", \"Parent Element\"])\n",
        "            writer.writerow([target_url, data, datetime.now(), element_type, parent_element])\n",
        "        print(f\"Content saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to CSV: {e}\")\n",
        "\n",
        "def save_to_db(data, db_name, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Saves the extracted content to a SQLite database.\n",
        "\n",
        "    Args:\n",
        "      data: The content to save.\n",
        "      db_name: The name of the database file.\n",
        "      element_type: The type of element the content was extracted from.\n",
        "      parent_element: The parent element of the extracted content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''CREATE TABLE IF NOT EXISTS url_content\n",
        "                                    (url TEXT, content TEXT, timestamp TEXT, element_type TEXT, parent_element TEXT)''')\n",
        "        cursor.execute(\"INSERT INTO url_content VALUES (?, ?, ?, ?, ?)\",\n",
        "                       (target_url, data, datetime.now(), element_type, parent_element))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(f\"Content saved to {db_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to database: {e}\")\n",
        "\n",
        "# @title Multi Page (Text Only)\n",
        "# @markdown **Requirements**\n",
        "# @markdown 1. Add `urls.txt` to `/content /` folder.\n",
        "# @markdown 2. Select which elements to parse below:\n",
        "element_type = 'body'  #@param ['section', 'body', 'article', 'div', 'p', 'span', 'main', 'header', 'footer', 'nav', 'aside'] {allow-input: true}\n",
        "# @markdown &nbsp;&nbsp;&nbsp;&nbsp;<sub><sup>*(parent_element not required)*</sub></sup>\n",
        "parent_element = ''  #@param [\"main\", \"div\", \"body\"] {allow-input: true}\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Read URLs from a text file\n",
        "    with open(\"urls.txt\", \"r\") as url_file:\n",
        "        urls = [url.strip() for url in url_file.readlines()]\n",
        "\n",
        "    print(f\"Starting scrape for {len(urls)} URLs...\\n\")\n",
        "\n",
        "    # Process each URL\n",
        "    for index, target_url in enumerate(urls, start=1):\n",
        "        try:\n",
        "            print(f\"[{index}/{len(urls)}] Fetching: {target_url}\")\n",
        "\n",
        "            # Introduce a random delay before each request (2-5 seconds)\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "\n",
        "            # Fetch HTML content\n",
        "            html = fetch_html(target_url)\n",
        "\n",
        "            if html:\n",
        "                # Parse content\n",
        "                content = parse_content(html, element_type, parent_element)\n",
        "\n",
        "                if content:\n",
        "                    # Save content to CSV and database with timestamps\n",
        "                    save_to_csv(content, 'url_content.csv', element_type, parent_element)\n",
        "                    save_to_db(content, 'url_content.db', element_type, parent_element)\n",
        "\n",
        "                    print(f\"✔ Content from {target_url} saved successfully at {datetime.now()}\\n\")\n",
        "                else:\n",
        "                    print(f\"⚠ No content found in <{element_type}> elements of {target_url}\\n\")\n",
        "            else:\n",
        "                print(f\"❌ Failed to fetch HTML from {target_url}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❗ Unexpected error processing {target_url}: {e}\\n\")\n",
        "            time.sleep(random.uniform(3, 6))  # Extra delay on errors to avoid bans"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EIPJI7tiCi1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import os  # Import the os module\n",
        "\n",
        "# Preprocess text: Remove special characters and normalize spaces\n",
        "def preprocess_text(text):\n",
        "    return re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', text)).lower()\n",
        "\n",
        "def fetch_html(url):\n",
        "    \"\"\"\n",
        "    Fetches the HTML content of a given URL.\n",
        "\n",
        "    Args:\n",
        "      url: The URL to fetch.\n",
        "\n",
        "    Returns:\n",
        "      The HTML content as a string, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "        return response.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_content(html, element_type, parent_element=None):\n",
        "    \"\"\"\n",
        "    Parses the HTML content to extract text and embedded hyperlinks from specified elements.\n",
        "\n",
        "    Args:\n",
        "        html: The HTML content to parse.\n",
        "        element_type: The type of element to extract text and hyperlinks from (e.g., 'div', 'p').\n",
        "        parent_element: (Optional) The parent element to search within.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the extracted text with embedded hyperlinks, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        if parent_element:\n",
        "            parent = soup.find(parent_element)\n",
        "            if not parent:\n",
        "                print(f\"No parent element <{parent_element}> found.\")\n",
        "                return None\n",
        "            elements = parent.find_all(element_type)\n",
        "        else:\n",
        "            elements = soup.find_all(element_type)\n",
        "\n",
        "        if elements:\n",
        "            content = \"\"\n",
        "            for element in elements:\n",
        "                # Extract text with embedded hyperlinks\n",
        "                for child in element.children:\n",
        "                    if child.name == 'a':\n",
        "                        href = child.get('href')\n",
        "                        if href:\n",
        "                            href = href.strip()\n",
        "                            href = href.replace('\\xa0', ' ')\n",
        "                            content += f\"<a href='{href}'>{child.text}</a>\"\n",
        "                    else:\n",
        "                        content += str(child)\n",
        "                content += \" \"  # Add space between elements\n",
        "            return content\n",
        "        else:\n",
        "            print(f\"No elements of type <{element_type}> found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data, filename, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Saves the extracted content to a CSV file.\n",
        "\n",
        "    Args:\n",
        "      data: The content to save.\n",
        "      filename: The name of the CSV file.\n",
        "      element_type: The type of element the content was extracted from.\n",
        "      parent_element: The parent element of the extracted content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the file exists\n",
        "        file_exists = os.path.isfile(filename)\n",
        "\n",
        "        with open(filename, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            if not file_exists:  # Write header only if file is new\n",
        "                writer.writerow([\"URL\", \"Content\", \"Timestamp\", \"Element Type\", \"Parent Element\"])\n",
        "            writer.writerow([target_url, data, datetime.now(), element_type, parent_element])\n",
        "        print(f\"Content saved to {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to CSV: {e}\")\n",
        "\n",
        "def save_to_db(data, db_name, element_type, parent_element):\n",
        "    \"\"\"\n",
        "    Saves the extracted content to a SQLite database.\n",
        "\n",
        "    Args:\n",
        "        data: The content to save (list of tuples).\n",
        "        db_name: The name of the database file.\n",
        "        element_type: The type of element the content was extracted from.\n",
        "        parent_element: The parent element of the extracted content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_name)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''CREATE TABLE IF NOT EXISTS url_content\n",
        "                            (url TEXT, text_content TEXT, href TEXT, timestamp TEXT, element_type TEXT, parent_element TEXT)''')\n",
        "        for text, href in data:  # Iterate over the list of tuples\n",
        "            cursor.execute(\"INSERT INTO url_content VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "                           (target_url, text, href, datetime.now(), element_type, parent_element))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "        print(f\"Content saved to {db_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving to database: {e}\")\n",
        "\n",
        "# @title Single Page (Text + <a> href)\n",
        "# @markdown **Step 1:** Select which elements to parse\n",
        "element_type = 'section'  #@param ['section', 'article', 'div', 'p', 'span', 'main', 'header', 'footer', 'nav', 'aside'] {allow-input: true}\n",
        "# @markdown &nbsp;&nbsp;&nbsp;&nbsp;<sub><sup>*(parent_element not required)*</sub></sup>\n",
        "parent_element = 'main'  #@param [\"main\", \"div\", \"body\"] {allow-input: true}\n",
        "# @markdown >\n",
        "# @markdown **Step 2:** Add the URL to be scraped\n",
        "target_url = \"https://www.stonebranch.com/it-automation-solutions/workload-automation\"  #@param {type:\"string\"}\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    html = fetch_html(target_url)\n",
        "    if html:\n",
        "        content = parse_content(html, element_type, parent_element)\n",
        "        if content:\n",
        "            # Save content to CSV and database with timestamps\n",
        "            save_to_csv(content, '/content/url_content.csv', element_type, parent_element)\n",
        "            save_to_db(content, '/content/url_content.db', element_type, parent_element)\n",
        "            print(f\"Content from {target_url} saved successfully at {datetime.now()}\")\n",
        "        else:\n",
        "            print(f\"No content found in <{element_type}> elements of {target_url}\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch HTML from {target_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "TZiBbBt8J_Wk",
        "outputId": "5c3e82fc-7f80-4519-8d03-530be420461b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content saved to /content/url_content.csv\n",
            "Error saving to database: not enough values to unpack (expected 2, got 1)\n",
            "Content from https://www.stonebranch.com/it-automation-solutions/workload-automation saved successfully at 2025-01-27 09:55:51.556619\n"
          ]
        }
      ]
    }
  ]
}